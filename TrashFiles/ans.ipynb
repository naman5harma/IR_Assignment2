{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Vector Space Model for Information Retrieval\n",
    "\n",
    "#This notebook implements an enhanced Vector Space Model (VSM) for information retrieval using the lnc.ltc weighting scheme.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedVSM:\n",
    "    \"\"\"\n",
    "    Enhanced Vector Space Model for Information Retrieval\n",
    "    \n",
    "    This class implements a vector space model using the lnc.ltc weighting scheme:\n",
    "    - lnc for documents: log term frequency, no IDF, cosine normalization\n",
    "    - ltc for queries: log term frequency, IDF, cosine normalization\n",
    "    \n",
    "    The vector space model represents documents and queries as vectors in a high-dimensional space,\n",
    "    where each dimension corresponds to a term in the corpus. Similarity between a query and a document\n",
    "    is computed using the cosine similarity of their respective vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_path, index_dir):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.index_dir = index_dir\n",
    "        self.term_index = defaultdict(lambda: {\"doc_freq\": 0, \"occurrences\": {}})\n",
    "        self.doc_magnitudes = {}\n",
    "        self.corpus_size = 0\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.doc_id_lookup = {}\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess the input text: lowercase, remove punctuation, tokenize, lemmatize, and remove stop words.\n",
    "        This step is crucial for both indexing and querying to ensure consistent term representation.\n",
    "        \"\"\"\n",
    "        # Convert to lowercase and remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        # Lemmatize and remove stop words\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in self.stop_words]\n",
    "\n",
    "    def construct_index(self):\n",
    "        \"\"\"\n",
    "        Construct the inverted index and calculate document magnitudes.\n",
    "        The inverted index maps each term to the documents it appears in, along with term frequencies.\n",
    "        Document magnitudes are used for cosine normalization during retrieval.\n",
    "        \"\"\"\n",
    "        for doc_id, filename in enumerate(sorted(os.listdir(self.corpus_path)), start=1):\n",
    "            if filename.endswith('.txt'):\n",
    "                self.corpus_size += 1\n",
    "                self.doc_id_lookup[str(doc_id)] = filename\n",
    "                filepath = os.path.join(self.corpus_path, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    content = file.read()\n",
    "                    tokens = self.preprocess_text(content)\n",
    "                    \n",
    "                    if not tokens:\n",
    "                        continue  # Skip empty documents\n",
    "                    \n",
    "                    term_freq = defaultdict(int)\n",
    "                    for token in tokens:\n",
    "                        term_freq[token] += 1\n",
    "                    \n",
    "                    doc_magnitude = 0\n",
    "                    for term, freq in term_freq.items():\n",
    "                        if str(doc_id) not in self.term_index[term][\"occurrences\"]:\n",
    "                            self.term_index[term][\"doc_freq\"] += 1\n",
    "                        self.term_index[term][\"occurrences\"][str(doc_id)] = freq\n",
    "                        \n",
    "                        # Calculate log term frequency for document vector\n",
    "                        log_tf = 1 + math.log10(freq)\n",
    "                        doc_magnitude += log_tf ** 2\n",
    "                    \n",
    "                    # Store the magnitude (length) of the document vector for later normalization\n",
    "                    self.doc_magnitudes[doc_id] = math.sqrt(doc_magnitude)\n",
    "\n",
    "    def persist_index(self):\n",
    "        \"\"\"Save the constructed index and related data to files for later use.\"\"\"\n",
    "        os.makedirs(self.index_dir, exist_ok=True)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, 'term_index.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.term_index, f)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, 'doc_magnitudes.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump({str(k): v for k, v in self.doc_magnitudes.items()}, f)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, 'corpus_size.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(str(self.corpus_size))\n",
    "\n",
    "        with open(os.path.join(self.index_dir, 'doc_id_lookup.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.doc_id_lookup, f)\n",
    "\n",
    "    def load_index(self):\n",
    "        \"\"\"Load the previously constructed index and related data from files.\"\"\"\n",
    "        with open(os.path.join(self.index_dir, 'term_index.json'), 'r', encoding='utf-8') as f:\n",
    "            self.term_index = json.load(f)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, 'doc_magnitudes.json'), 'r', encoding='utf-8') as f:\n",
    "            self.doc_magnitudes = {int(k): v for k, v in json.load(f).items()}\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, 'corpus_size.txt'), 'r', encoding='utf-8') as f:\n",
    "            self.corpus_size = int(f.read().strip())\n",
    "\n",
    "        with open(os.path.join(self.index_dir, 'doc_id_lookup.json'), 'r', encoding='utf-8') as f:\n",
    "            self.doc_id_lookup = json.load(f)\n",
    "\n",
    "    def execute_query(self, query):\n",
    "        \"\"\"\n",
    "        Execute a search query using the vector space model.\n",
    "        \n",
    "        This method implements the core of the vector space model:\n",
    "        1. Preprocess the query\n",
    "        2. Compute the query vector using the ltc scheme\n",
    "        3. Compute document vectors using the lnc scheme\n",
    "        4. Calculate cosine similarity between the query vector and document vectors\n",
    "        5. Rank documents based on their similarity scores\n",
    "        \"\"\"\n",
    "        query_terms = self.preprocess_text(query)\n",
    "        query_vector = defaultdict(float)\n",
    "        doc_vectors = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "        # Compute query vector weights (ltc scheme)\n",
    "        for term in set(query_terms):\n",
    "            if term in self.term_index:\n",
    "                # l: log term frequency for query\n",
    "                tf = 1 + math.log10(query_terms.count(term))\n",
    "                # t: inverse document frequency\n",
    "                idf = math.log10(self.corpus_size / self.term_index[term][\"doc_freq\"])\n",
    "                # Combine log tf and idf\n",
    "                query_vector[term] = tf * idf\n",
    "\n",
    "        # c: Cosine normalization for query vector\n",
    "        query_magnitude = math.sqrt(sum(weight ** 2 for weight in query_vector.values()))\n",
    "        if query_magnitude == 0:\n",
    "            return []  # No valid terms in query\n",
    "        for term in query_vector:\n",
    "            query_vector[term] /= query_magnitude\n",
    "\n",
    "        # Compute document vector weights (lnc scheme)\n",
    "        for term in query_vector:\n",
    "            for doc_id_str, freq in self.term_index[term][\"occurrences\"].items():\n",
    "                doc_id = int(doc_id_str)\n",
    "                if doc_id in self.doc_magnitudes:\n",
    "                    # l: log term frequency for documents\n",
    "                    doc_vectors[doc_id][term] = 1 + math.log10(freq)\n",
    "\n",
    "        # c: Cosine normalization for document vectors\n",
    "        for doc_id, vector in doc_vectors.items():\n",
    "            magnitude = self.doc_magnitudes[doc_id]\n",
    "            for term in vector:\n",
    "                vector[term] /= magnitude\n",
    "\n",
    "        # Compute cosine similarity between query and documents\n",
    "        similarity_scores = {}\n",
    "        for doc_id, doc_vector in doc_vectors.items():\n",
    "            # Dot product of query and document vectors\n",
    "            score = sum(query_vector[term] * doc_vector[term] for term in query_vector if term in doc_vector)\n",
    "            similarity_scores[doc_id] = score\n",
    "\n",
    "        # Rank documents by similarity score (descending) and then by document ID (ascending)\n",
    "        ranked_docs = sorted(similarity_scores.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "        # Return top 10 results with document names and scores\n",
    "        return [(self.doc_id_lookup.get(str(doc_id), f\"Unknown-{doc_id}\"), score) for doc_id, score in ranked_docs[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Enhanced Vector Space Model for Information Retrieval\")\n",
    "    parser.add_argument('--corpus', default='corpus', help='Path to the corpus directory (default: corpus)')\n",
    "    parser.add_argument('--index', default='index', help='Path to the index directory (default: index)')\n",
    "    parser.add_argument('--build', action='store_true', help='Build the index')\n",
    "    parser.add_argument('--search', action='store_true', help='Perform search')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    vsm = EnhancedVSM(args.corpus, args.index)\n",
    "\n",
    "    if args.build:\n",
    "        print(\"Constructing index...\")\n",
    "        vsm.construct_index()\n",
    "        vsm.persist_index()\n",
    "        print(\"Index constructed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 19 (3043414436.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 20\u001b[1;36m\u001b[0m\n\u001b[1;33m    main()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 19\n"
     ]
    }
   ],
   "source": [
    "    if args.search:\n",
    "        print(\"Loading index...\")\n",
    "        vsm.load_index()\n",
    "        print(\"Index loaded successfully.\")\n",
    "\n",
    "        while True:\n",
    "            query = input(\"\\nEnter your search query (or 'exit' to quit): \")\n",
    "            if query.lower() == 'exit':\n",
    "                break\n",
    "\n",
    "            results = vsm.execute_query(query)\n",
    "            if results:\n",
    "                print(\"\\nTop 10 most relevant documents:\")\n",
    "                for i, (filename, score) in enumerate(results, 1):\n",
    "                    print(f\"{i}. {filename} (Similarity: {score:.4f})\")\n",
    "            else:\n",
    "                print(\"No relevant documents found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
