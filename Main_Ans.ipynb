{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Vector Space Model for Information Retrieval\n",
    "\n",
    "#This notebook implements an enhanced Vector Space Model (VSM) for information retrieval using the lnc.ltc weighting scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EnhancedVSM Class\n",
    "\n",
    "#This class implements the core functionality of our Vector Space Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedVSM:\n",
    "    def __init__(self, corpus_path, index_dir):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.index_dir = index_dir\n",
    "        self.term_index = defaultdict(lambda: {\"doc_freq\": 0, \"occurrences\": {}})\n",
    "        self.doc_magnitudes = {}\n",
    "        self.corpus_size = 0\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.doc_id_lookup = {}\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "        tokens = word_tokenize(text)\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in self.stop_words]\n",
    "\n",
    "    def construct_index(self):\n",
    "        for doc_id, filename in enumerate(sorted(os.listdir(self.corpus_path)), start=1):\n",
    "            if filename.endswith('.txt'):\n",
    "                self.corpus_size += 1\n",
    "                self.doc_id_lookup[str(doc_id)] = filename\n",
    "                filepath = os.path.join(self.corpus_path, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    content = file.read()\n",
    "                    tokens = self.preprocess_text(content)\n",
    "                    \n",
    "                    if not tokens:\n",
    "                        continue  # Skip empty documents\n",
    "                    \n",
    "                    term_freq = defaultdict(int)\n",
    "                    for token in tokens:\n",
    "                        term_freq[token] += 1\n",
    "                    \n",
    "                    doc_magnitude = 0\n",
    "                    for term, freq in term_freq.items():\n",
    "                        if str(doc_id) not in self.term_index[term][\"occurrences\"]:\n",
    "                            self.term_index[term][\"doc_freq\"] += 1\n",
    "                        self.term_index[term][\"occurrences\"][str(doc_id)] = freq\n",
    "                        \n",
    "                        log_tf = 1 + math.log10(freq)\n",
    "                        doc_magnitude += log_tf ** 2\n",
    "                    \n",
    "                    self.doc_magnitudes[doc_id] = math.sqrt(doc_magnitude)\n",
    "\n",
    "    def persist_index(self):\n",
    "        os.makedirs(self.index_dir, exist_ok=True)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, 'term_index.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.term_index, f)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, 'doc_magnitudes.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump({str(k): v for k, v in self.doc_magnitudes.items()}, f)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, 'corpus_size.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(str(self.corpus_size))\n",
    "\n",
    "        with open(os.path.join(self.index_dir, 'doc_id_lookup.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.doc_id_lookup, f)\n",
    "\n",
    "    def load_index(self):\n",
    "        with open(os.path.join(self.index_dir, 'term_index.json'), 'r', encoding='utf-8') as f:\n",
    "            self.term_index = json.load(f)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, 'doc_magnitudes.json'), 'r', encoding='utf-8') as f:\n",
    "            self.doc_magnitudes = {int(k): v for k, v in json.load(f).items()}\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, 'corpus_size.txt'), 'r', encoding='utf-8') as f:\n",
    "            self.corpus_size = int(f.read().strip())\n",
    "\n",
    "        with open(os.path.join(self.index_dir, 'doc_id_lookup.json'), 'r', encoding='utf-8') as f:\n",
    "            self.doc_id_lookup = json.load(f)\n",
    "\n",
    "    def execute_query(self, query):\n",
    "        query_terms = self.preprocess_text(query)\n",
    "        query_vector = defaultdict(float)\n",
    "        doc_vectors = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "        for term in set(query_terms):\n",
    "            if term in self.term_index:\n",
    "                tf = 1 + math.log10(query_terms.count(term))\n",
    "                idf = math.log10(self.corpus_size / self.term_index[term][\"doc_freq\"])\n",
    "                query_vector[term] = tf * idf\n",
    "\n",
    "        query_magnitude = math.sqrt(sum(weight ** 2 for weight in query_vector.values()))\n",
    "        if query_magnitude == 0:\n",
    "            return []  # No valid terms in query\n",
    "        for term in query_vector:\n",
    "            query_vector[term] /= query_magnitude\n",
    "\n",
    "        for term in query_vector:\n",
    "            for doc_id_str, freq in self.term_index[term][\"occurrences\"].items():\n",
    "                doc_id = int(doc_id_str)\n",
    "                if doc_id in self.doc_magnitudes:\n",
    "                    doc_vectors[doc_id][term] = 1 + math.log10(freq)\n",
    "\n",
    "        for doc_id, vector in doc_vectors.items():\n",
    "            magnitude = self.doc_magnitudes[doc_id]\n",
    "            for term in vector:\n",
    "                vector[term] /= magnitude\n",
    "\n",
    "        similarity_scores = {}\n",
    "        for doc_id, doc_vector in doc_vectors.items():\n",
    "            score = sum(query_vector[term] * doc_vector[term] for term in query_vector if term in doc_vector)\n",
    "            similarity_scores[doc_id] = score\n",
    "\n",
    "        ranked_docs = sorted(similarity_scores.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "        return [(self.doc_id_lookup.get(str(doc_id), f\"Unknown-{doc_id}\"), score) for doc_id, score in ranked_docs[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the Index\n",
    "\n",
    "#Run this cell to build the index for your corpus. Make sure your corpus is in a folder named 'corpus' in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing index...\n",
      "Index constructed and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "vsm = EnhancedVSM('corpus', 'index')\n",
    "print(\"Constructing index...\")\n",
    "vsm.construct_index()\n",
    "vsm.persist_index()\n",
    "print(\"Index constructed and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the Index\n",
    "\n",
    "#If you've already built the index, you can load it using this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index...\n",
      "Index loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "vsm = EnhancedVSM('corpus', 'index')\n",
    "print(\"Loading index...\")\n",
    "vsm.load_index()\n",
    "print(\"Index loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performing Queries\n",
    "\n",
    "#Use this cell to perform queries on your corpus. You can modify the queries or add more as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Developing your Zomato business account and profile is a great way to boost your restaurant's online reputation'\n",
      "Top 10 most relevant documents:\n",
      "1. zomato.txt (Similarity: 0.2036)\n",
      "2. swiggy.txt (Similarity: 0.1213)\n",
      "3. instagram.txt (Similarity: 0.0564)\n",
      "4. messenger.txt (Similarity: 0.0556)\n",
      "5. youtube.txt (Similarity: 0.0454)\n",
      "6. reddit.txt (Similarity: 0.0440)\n",
      "7. bing.txt (Similarity: 0.0415)\n",
      "8. flipkart.txt (Similarity: 0.0396)\n",
      "9. paypal.txt (Similarity: 0.0389)\n",
      "10. HP.txt (Similarity: 0.0389)\n",
      "\n",
      "Query: 'Warwickshire, came from an ancient family and was the heiress to some land'\n",
      "Top 10 most relevant documents:\n",
      "1. shakespeare.txt (Similarity: 0.1202)\n",
      "2. levis.txt (Similarity: 0.0241)\n",
      "3. nike.txt (Similarity: 0.0183)\n",
      "4. Adobe.txt (Similarity: 0.0158)\n",
      "5. zomato.txt (Similarity: 0.0149)\n",
      "6. huawei.txt (Similarity: 0.0136)\n",
      "7. skype.txt (Similarity: 0.0121)\n",
      "8. blackberry.txt (Similarity: 0.0114)\n",
      "9. reliance.txt (Similarity: 0.0105)\n",
      "10. Dell.txt (Similarity: 0.0104)\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Developing your Zomato business account and profile is a great way to boost your restaurant's online reputation\",\n",
    "    \"Warwickshire, came from an ancient family and was the heiress to some land\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results = vsm.execute_query(query)\n",
    "    if results:\n",
    "        print(\"Top 10 most relevant documents:\")\n",
    "        for i, (filename, score) in enumerate(results, 1):\n",
    "            print(f\"{i}. {filename} (Similarity: {score:.4f})\")\n",
    "    else:\n",
    "        print(\"No relevant documents found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interactive Search\n",
    "\n",
    "#Use this cell to perform interactive searches on your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most relevant documents:\n",
      "1. shakespeare.txt (Similarity: 0.1202)\n",
      "2. levis.txt (Similarity: 0.0241)\n",
      "3. nike.txt (Similarity: 0.0183)\n",
      "4. Adobe.txt (Similarity: 0.0158)\n",
      "5. zomato.txt (Similarity: 0.0149)\n",
      "6. huawei.txt (Similarity: 0.0136)\n",
      "7. skype.txt (Similarity: 0.0121)\n",
      "8. blackberry.txt (Similarity: 0.0114)\n",
      "9. reliance.txt (Similarity: 0.0105)\n",
      "10. Dell.txt (Similarity: 0.0104)\n",
      "\n",
      "No relevant documents found.\n",
      "\n",
      "No relevant documents found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"Enter your search query (or 'exit' to quit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    results = vsm.execute_query(query)\n",
    "    if results:\n",
    "        print(\"\\nTop 10 most relevant documents:\")\n",
    "        for i, (filename, score) in enumerate(results, 1):\n",
    "            print(f\"{i}. {filename} (Similarity: {score:.4f})\")\n",
    "    else:\n",
    "        print(\"No relevant documents found.\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
